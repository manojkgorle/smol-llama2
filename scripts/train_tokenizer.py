"""
Script to download TinyStories dataset and train a BPE tokenizer.

USAGE:
    python scripts/train_tokenizer.py

This script:
  1. Downloads the TinyStories dataset from HuggingFace
  2. Exports the training text to a flat file
  3. Trains a SentencePiece BPE tokenizer with byte-fallback
  4. Verifies the tokenizer works correctly

The tokenizer model is saved to data/tokenizer.model and is used by
the training and inference pipelines.

WHAT IS TINYSTORIES?
  TinyStories (Eldan & Li, 2023) is a dataset of ~2.1 million short
  stories generated by GPT-3.5/4. Each story uses simple vocabulary
  and grammar suitable for young children.

  Why TinyStories for our tiny model:
    - Simple language patterns that 15M params can learn
    - Large enough (300M+ tokens) to train without overfitting
    - Stories are coherent, so we can evaluate quality by reading output
    - Fast to download and process
"""

import os
import sys
import argparse
import tempfile

# Add project root to path so we can import llama_vc
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from llama_vc.tokenizer import train_tokenizer, Tokenizer


def download_and_export_tinystories(
    output_file: str,
    max_stories: int = -1,
) -> None:
    """
    Download TinyStories from HuggingFace and export to a plain text file.

    Each story is separated by a newline. The text file is used as input
    to train the SentencePiece tokenizer.

    Args:
        output_file: Path to write the exported text.
        max_stories: Maximum number of stories to export (-1 = all).
    """
    try:
        from datasets import load_dataset
    except ImportError:
        print("ERROR: 'datasets' package not found.")
        print("Install with: pip install datasets")
        sys.exit(1)

    print("Downloading TinyStories dataset from HuggingFace...")
    print("(This may take a few minutes on first run)")

    # Load the dataset. TinyStories is available as "roneneldan/TinyStories"
    # The train split contains ~2.1M stories.
    dataset = load_dataset("roneneldan/TinyStories", split="train")

    print(f"Dataset loaded: {len(dataset)} stories")

    # Export to plain text file
    # Each story on its own line (SentencePiece treats newlines as sentence boundaries)
    os.makedirs(os.path.dirname(output_file) or ".", exist_ok=True)

    n_stories = 0
    with open(output_file, "w", encoding="utf-8") as f:
        for i, example in enumerate(dataset):
            if max_stories > 0 and i >= max_stories:
                break
            # The text field contains the story
            text = example["text"].strip()
            if text:
                f.write(text + "\n")
                n_stories += 1

            if (i + 1) % 100_000 == 0:
                print(f"  Exported {i + 1:,} stories...")

    file_size_mb = os.path.getsize(output_file) / 1024**2
    print(f"Exported {n_stories:,} stories to {output_file} ({file_size_mb:.1f} MB)")


def verify_tokenizer(model_path: str) -> None:
    """
    Run verification tests on the trained tokenizer.

    Tests:
      1. Basic encode/decode roundtrip
      2. Byte-fallback for non-ASCII characters
      3. Special token handling
      4. Vocabulary size
    """
    print("\n" + "=" * 50)
    print("Verifying tokenizer...")
    print("=" * 50)

    tok = Tokenizer(model_path)

    # Test 1: Basic roundtrip
    test_texts = [
        "Once upon a time, there was a little cat.",
        "The sun was shining bright in the blue sky.",
        "She smiled and said hello to her friend.",
        "1 + 2 = 3",
        "ABCDEFG",
    ]
    for text in test_texts:
        tokens = tok.encode(text, bos=False, eos=False)
        decoded = tok.decode(tokens)
        status = "PASS" if decoded == text else "FAIL"
        print(f"  [{status}] Roundtrip: '{text}' ({len(tokens)} tokens)")
        if decoded != text:
            print(f"         Got: '{decoded}'")

    # Test 2: Byte-fallback (non-ASCII)
    unicode_texts = [
        "cafÃ©",       # French accent
        "naÃ¯ve",      # diaeresis
        "æ—¥æœ¬èªž",     # Japanese
        "ðŸŽ‰ðŸŽŠ",      # Emoji
    ]
    for text in unicode_texts:
        tokens = tok.encode(text, bos=False, eos=False)
        decoded = tok.decode(tokens)
        has_unk = tok.unk_id in tokens
        status = "PASS" if not has_unk else "WARN"
        print(f"  [{status}] Byte-fallback: '{text}' â†’ {len(tokens)} tokens, unk={has_unk}")

    # Test 3: Special tokens
    tokens_with_special = tok.encode("Hello", bos=True, eos=True)
    assert tokens_with_special[0] == tok.bos_id, "BOS should be first token"
    assert tokens_with_special[-1] == tok.eos_id, "EOS should be last token"
    print(f"  [PASS] Special tokens: BOS={tok.bos_id}, EOS={tok.eos_id}")

    # Test 4: Vocabulary info
    print(f"\n  Vocabulary size: {tok.vocab_size}")
    print(f"  Sample tokens:")
    for i in [0, 1, 2, 3, 100, 500, 1000, 2000]:
        if i < tok.vocab_size:
            piece = tok.id_to_piece(i)
            print(f"    {i:>5d}: '{piece}'")

    print("\nTokenizer verification complete!")


def main():
    parser = argparse.ArgumentParser(
        description="Train a BPE tokenizer on TinyStories"
    )
    parser.add_argument(
        "--vocab-size", type=int, default=4096,
        help="Vocabulary size (default: 4096)"
    )
    parser.add_argument(
        "--data-dir", type=str, default="data/",
        help="Directory for data files (default: data/)"
    )
    parser.add_argument(
        "--max-stories", type=int, default=-1,
        help="Max stories to use for tokenizer training (-1 = all, "
             "use e.g. 100000 for faster training during development)"
    )
    args = parser.parse_args()

    # Step 1: Download and export TinyStories
    text_file = os.path.join(args.data_dir, "tinystories_train.txt")
    if not os.path.exists(text_file):
        download_and_export_tinystories(text_file, max_stories=args.max_stories)
    else:
        print(f"Using existing text file: {text_file}")

    # Step 2: Train tokenizer
    model_prefix = os.path.join(args.data_dir, "tokenizer")
    model_path = f"{model_prefix}.model"

    if not os.path.exists(model_path):
        train_tokenizer(
            input_file=text_file,
            model_prefix=model_prefix,
            vocab_size=args.vocab_size,
        )
    else:
        print(f"Using existing tokenizer: {model_path}")

    # Step 3: Verify
    verify_tokenizer(model_path)


if __name__ == "__main__":
    main()
