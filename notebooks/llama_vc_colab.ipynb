{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA from Scratch: Train & Generate on Google Colab\n",
    "\n",
    "This notebook trains a **~15M parameter LLaMA model** on the [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset, end-to-end:\n",
    "\n",
    "1. **Train a BPE tokenizer** (SentencePiece, 4096 vocab)\n",
    "2. **Tokenize the dataset** into memory-mapped binary files\n",
    "3. **Train the model** with mixed precision, gradient accumulation, cosine LR schedule\n",
    "4. **Evaluate** on the validation set (loss + perplexity)\n",
    "5. **Generate text** from prompts with temperature/top-k/top-p sampling\n",
    "6. **Save & download** the trained checkpoint\n",
    "\n",
    "**Architecture:** Decoder-only transformer following LLaMA (Meta AI)\n",
    "- RMSNorm (pre-normalization)\n",
    "- Rotary Positional Embeddings (RoPE)\n",
    "- SwiGLU activation in FFN\n",
    "- Grouped Query Attention (GQA, 6 query heads / 2 KV heads)\n",
    "- KV cache for efficient inference\n",
    "\n",
    "**Requirements:** A Colab GPU runtime (T4 or A100). Go to *Runtime > Change runtime type > GPU*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Setup: Install Dependencies & Clone Repo\n!pip install -q torch sentencepiece datasets tqdm\n!git clone -q https://github.com/manojkgorle/llama2.git\n%cd llama2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Configuration — Edit these parameters!\n",
    "\n",
    "# ── Training ─────────────────────────────────────────────────────────────\n",
    "TRAINING_STEPS = 3000          # Total optimizer steps (~15 min on T4)\n",
    "BATCH_SIZE = 64                # Sequences per micro-batch\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Effective batch = 64 * 4 = 256 sequences\n",
    "LEARNING_RATE = 3e-4           # Peak LR (after warmup)\n",
    "MIN_LEARNING_RATE = 3e-5       # Floor LR (10% of peak)\n",
    "WARMUP_STEPS = 200             # Linear warmup steps\n",
    "WEIGHT_DECAY = 0.1             # AdamW weight decay\n",
    "MAX_GRAD_NORM = 1.0            # Gradient clipping\n",
    "\n",
    "# ── Evaluation & Logging ─────────────────────────────────────────────────\n",
    "EVAL_INTERVAL = 500            # Evaluate every N steps\n",
    "EVAL_STEPS = 20                # Batches per evaluation\n",
    "LOG_INTERVAL = 50              # Print loss every N steps\n",
    "SAVE_INTERVAL = 1000           # Save checkpoint every N steps\n",
    "\n",
    "# ── Model Architecture ──────────────────────────────────────────────────\n",
    "VOCAB_SIZE = 4096\n",
    "DIM = 384\n",
    "N_LAYERS = 8\n",
    "N_HEADS = 6\n",
    "N_KV_HEADS = 2\n",
    "MAX_SEQ_LEN = 512\n",
    "HIDDEN_DIM = 1024\n",
    "\n",
    "# ── Paths ────────────────────────────────────────────────────────────────\n",
    "DATA_DIR = \"data/\"\n",
    "CHECKPOINT_DIR = \"checkpoints/\"\n",
    "TOKENIZER_PREFIX = \"data/tokenizer\"  # produces data/tokenizer.model\n",
    "SEED = 42\n",
    "\n",
    "print(f\"Training for {TRAINING_STEPS} steps\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS} sequences\")\n",
    "print(f\"Tokens per step: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS * MAX_SEQ_LEN:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Device Detection\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    props = torch.cuda.get_device_properties(device)\n",
    "    print(f\"GPU: {props.name}\")\n",
    "    print(f\"VRAM: {props.total_mem / 1024**3:.1f} GB\")\n",
    "    print(f\"Compute Capability: {props.major}.{props.minor}\")\n",
    "    print(f\"BF16 Support: {torch.cuda.is_bf16_supported()}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Device: MPS (Apple Silicon)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"WARNING: No GPU detected! Training will be very slow.\")\n",
    "    print(\"Go to Runtime > Change runtime type > GPU\")\n",
    "\n",
    "print(f\"\\nDevice: {device}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import Modules\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from llama_vc.config import ModelConfig\n",
    "from llama_vc.model import LLaMA\n",
    "from llama_vc.tokenizer import Tokenizer, train_tokenizer\n",
    "from llama_vc.dataset import prepare_data, create_dataloader\n",
    "from llama_vc.device import (\n",
    "    get_dtype, get_autocast_context,\n",
    "    get_grad_scaler, get_memory_usage,\n",
    ")\n",
    "from llama_vc.generate import generate\n",
    "from llama_vc.train import get_lr, evaluate\n",
    "from llama_vc.utils import (\n",
    "    set_seed, count_parameters, print_model_summary,\n",
    "    save_checkpoint, load_checkpoint,\n",
    ")\n",
    "\n",
    "print(\"All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Train Tokenizer\n",
    "\n",
    "We train a **BPE tokenizer** (Byte-Pair Encoding) with SentencePiece on the TinyStories training data.\n",
    "\n",
    "- **Vocab size:** 4096 tokens (small, matching our tiny model)\n",
    "- **Byte fallback:** Unknown characters are encoded as UTF-8 bytes (no `<unk>` tokens)\n",
    "- **Special tokens:** `<s>` (BOS, id=1), `</s>` (EOS, id=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 1: Train Tokenizer\n",
    "from llama_vc.dataset import download_tinystories\n",
    "\n",
    "# Download TinyStories training data (needed for tokenizer training)\n",
    "train_text_path = download_tinystories(DATA_DIR)\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer_model_path = TOKENIZER_PREFIX + \".model\"\n",
    "if os.path.exists(tokenizer_model_path):\n",
    "    print(f\"Tokenizer already exists: {tokenizer_model_path}\")\n",
    "else:\n",
    "    train_tokenizer(\n",
    "        input_file=train_text_path,\n",
    "        model_prefix=TOKENIZER_PREFIX,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "    )\n",
    "\n",
    "# Load and verify\n",
    "tokenizer = Tokenizer(tokenizer_model_path)\n",
    "print(f\"\\nTokenizer loaded: vocab_size={tokenizer.vocab_size}\")\n",
    "\n",
    "# Test roundtrip\n",
    "test_text = \"Once upon a time, there was a little cat.\"\n",
    "tokens = tokenizer.encode(test_text, bos=True, eos=True)\n",
    "decoded = tokenizer.decode(tokens)\n",
    "print(f\"Encode: '{test_text}'\")\n",
    "print(f\"  -> {tokens[:15]}... ({len(tokens)} tokens)\")\n",
    "print(f\"Decode: '{decoded}'\")\n",
    "assert decoded.strip() == test_text, \"Roundtrip failed!\"\n",
    "print(\"Roundtrip: PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 2: Prepare Data (Tokenize to Binary)\n",
    "\n",
    "# Download val split and tokenize both splits to .bin files\n",
    "train_bin, val_bin = prepare_data(DATA_DIR, tokenizer)\n",
    "\n",
    "# Print dataset stats\n",
    "train_tokens = os.path.getsize(train_bin) // 2  # uint16 = 2 bytes\n",
    "val_tokens = os.path.getsize(val_bin) // 2\n",
    "print(f\"\\nTrain tokens: {train_tokens:,}\")\n",
    "print(f\"Val tokens:   {val_tokens:,}\")\n",
    "print(f\"Train file:   {os.path.getsize(train_bin) / 1024**2:.1f} MB\")\n",
    "print(f\"Val file:     {os.path.getsize(val_bin) / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 3: Create Model\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# Build model config (update vocab_size from tokenizer)\n",
    "model_config = ModelConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    dim=DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    n_kv_heads=N_KV_HEADS,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    ")\n",
    "model_config.validate()\n",
    "\n",
    "# Create model\n",
    "model = LLaMA(model_config).to(device)\n",
    "n_params = count_parameters(model)\n",
    "print(f\"\\nModel parameters: {n_params:,}\")\n",
    "print_model_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train\n",
    "\n",
    "The training loop below runs for `TRAINING_STEPS` optimizer steps with:\n",
    "- **Mixed precision** (bf16 on Ampere+, fp16 on T4, fp32 on CPU)\n",
    "- **Gradient accumulation** (effective batch = `BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS`)\n",
    "- **Cosine LR schedule** with linear warmup\n",
    "- **Gradient clipping** to prevent explosions\n",
    "- **Periodic evaluation** on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 4: Train the Model\n",
    "\n",
    "# ── Setup ────────────────────────────────────────────────────────────────\n",
    "dtype = get_dtype(\"auto\", device)\n",
    "autocast_ctx = get_autocast_context(device, dtype)\n",
    "scaler = get_grad_scaler(device, dtype)\n",
    "print(f\"Training dtype: {dtype}\")\n",
    "print(f\"GradScaler: {'enabled' if scaler else 'disabled'}\")\n",
    "\n",
    "# ── DataLoaders ──────────────────────────────────────────────────────────\n",
    "train_loader = create_dataloader(\n",
    "    train_bin, seq_len=MAX_SEQ_LEN, batch_size=BATCH_SIZE,\n",
    "    shuffle=True, pin_memory=(device.type != \"cpu\"),\n",
    ")\n",
    "val_loader = create_dataloader(\n",
    "    val_bin, seq_len=MAX_SEQ_LEN, batch_size=BATCH_SIZE,\n",
    "    shuffle=False, pin_memory=(device.type != \"cpu\"),\n",
    ")\n",
    "\n",
    "# ── Optimizer ────────────────────────────────────────────────────────────\n",
    "optimizer = model.configure_optimizers(\n",
    "    learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY,\n",
    "    betas=(0.9, 0.95), device=device,\n",
    ")\n",
    "\n",
    "# ── Training Loop ────────────────────────────────────────────────────────\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "model.train()\n",
    "train_iter = iter(train_loader)\n",
    "tokens_per_step = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS * MAX_SEQ_LEN\n",
    "best_val_loss = float(\"inf\")\n",
    "train_losses = []\n",
    "\n",
    "print(f\"\\nStarting training: {TRAINING_STEPS} steps\")\n",
    "print(f\"Effective batch: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS} sequences = {tokens_per_step:,} tokens/step\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "pbar = tqdm(range(TRAINING_STEPS), desc=\"Training\", unit=\"step\")\n",
    "for step in pbar:\n",
    "    step_start = time.perf_counter()\n",
    "\n",
    "    # ── Learning Rate Schedule ───────────────────────────────────────────\n",
    "    lr = get_lr(step, WARMUP_STEPS, TRAINING_STEPS, LEARNING_RATE, MIN_LEARNING_RATE)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "    # ── Gradient Accumulation ────────────────────────────────────────────\n",
    "    accumulated_loss = 0.0\n",
    "    for micro_step in range(GRADIENT_ACCUMULATION_STEPS):\n",
    "        try:\n",
    "            x, y = next(train_iter)\n",
    "        except StopIteration:\n",
    "            train_iter = iter(train_loader)\n",
    "            x, y = next(train_iter)\n",
    "\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        with autocast_ctx:\n",
    "            _, loss = model(x, targets=y)\n",
    "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        accumulated_loss += loss.item()\n",
    "\n",
    "    # ── Gradient Clipping + Optimizer Step ───────────────────────────────\n",
    "    if scaler is not None:\n",
    "        scaler.unscale_(optimizer)\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "\n",
    "    if scaler is not None:\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    else:\n",
    "        optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # ── Timing ───────────────────────────────────────────────────────────\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    step_time = time.perf_counter() - step_start\n",
    "    tok_per_sec = tokens_per_step / step_time\n",
    "\n",
    "    # ── Progress Bar ─────────────────────────────────────────────────────\n",
    "    train_losses.append(accumulated_loss)\n",
    "    pbar.set_postfix(loss=f\"{accumulated_loss:.4f}\", lr=f\"{lr:.2e}\", tps=f\"{tok_per_sec:,.0f}\")\n",
    "\n",
    "    # ── Log ──────────────────────────────────────────────────────────────\n",
    "    if step % LOG_INTERVAL == 0:\n",
    "        mem = get_memory_usage(device)\n",
    "        print(\n",
    "            f\"step {step:>5d}/{TRAINING_STEPS} | \"\n",
    "            f\"loss {accumulated_loss:.4f} | \"\n",
    "            f\"lr {lr:.2e} | \"\n",
    "            f\"{tok_per_sec:>8,.0f} tok/s | \"\n",
    "            f\"mem {mem['allocated_mb']:>6.0f} MB\"\n",
    "        )\n",
    "\n",
    "    # ── Evaluation ───────────────────────────────────────────────────────\n",
    "    if step > 0 and step % EVAL_INTERVAL == 0:\n",
    "        val_loss = evaluate(model, val_loader, device, autocast_ctx, max_steps=EVAL_STEPS)\n",
    "        perplexity = math.exp(val_loss)\n",
    "        print(f\"{'─' * 60}\")\n",
    "        print(f\"EVAL step {step} | val_loss {val_loss:.4f} | perplexity {perplexity:.2f}\")\n",
    "        print(f\"{'─' * 60}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_checkpoint(\n",
    "                model, optimizer, step, val_loss,\n",
    "                model_config.to_dict(), {},\n",
    "                os.path.join(CHECKPOINT_DIR, \"best.pt\"),\n",
    "            )\n",
    "        model.train()\n",
    "\n",
    "    # ── Periodic Checkpoint ──────────────────────────────────────────────\n",
    "    if step > 0 and step % SAVE_INTERVAL == 0:\n",
    "        save_checkpoint(\n",
    "            model, optimizer, step, best_val_loss,\n",
    "            model_config.to_dict(), {},\n",
    "            os.path.join(CHECKPOINT_DIR, f\"step_{step:06d}.pt\"),\n",
    "        )\n",
    "\n",
    "# ── Final ────────────────────────────────────────────────────────────────\n",
    "print(\"\\nTraining complete!\")\n",
    "final_path = os.path.join(CHECKPOINT_DIR, \"final.pt\")\n",
    "save_checkpoint(\n",
    "    model, optimizer, TRAINING_STEPS, best_val_loss,\n",
    "    model_config.to_dict(), {}, final_path,\n",
    ")\n",
    "print(f\"Final checkpoint saved: {final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 5: Evaluate\n",
    "\n",
    "val_loss = evaluate(model, val_loader, device, autocast_ctx, max_steps=EVAL_STEPS)\n",
    "perplexity = math.exp(val_loss)\n",
    "\n",
    "print(f\"Final Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Final Perplexity:      {perplexity:.2f}\")\n",
    "print(f\"Best Validation Loss:  {best_val_loss:.4f}\")\n",
    "print(f\"Best Perplexity:       {math.exp(best_val_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 6: Generate Text\n",
    "\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The little dog\",\n",
    "    \"She looked at the sky and\",\n",
    "    \"One day, a boy named Tom\",\n",
    "]\n",
    "\n",
    "for temp in [0.7, 1.0]:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Temperature = {temp}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    for prompt in prompts:\n",
    "        text = generate(\n",
    "            model, tokenizer, prompt,\n",
    "            max_new_tokens=150, temperature=temp,\n",
    "            top_k=40, top_p=0.9, device=device,\n",
    "        )\n",
    "        print(f\"\\n--- Prompt: \\\"{prompt}\\\" ---\")\n",
    "        print(text)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 7: Save & Download Model\n",
    "\n",
    "# Save model config alongside checkpoint\n",
    "model_config.save(os.path.join(CHECKPOINT_DIR, \"model_config.json\"))\n",
    "print(f\"Model config saved to {CHECKPOINT_DIR}model_config.json\")\n",
    "\n",
    "# List all checkpoints\n",
    "print(\"\\nCheckpoints:\")\n",
    "for f in sorted(os.listdir(CHECKPOINT_DIR)):\n",
    "    path = os.path.join(CHECKPOINT_DIR, f)\n",
    "    size_mb = os.path.getsize(path) / 1024**2\n",
    "    print(f\"  {f}: {size_mb:.1f} MB\")\n",
    "\n",
    "# Download the best checkpoint (Colab only)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    best_path = os.path.join(CHECKPOINT_DIR, \"best.pt\")\n",
    "    if os.path.exists(best_path):\n",
    "        print(f\"\\nDownloading {best_path}...\")\n",
    "        files.download(best_path)\n",
    "    else:\n",
    "        print(f\"\\nDownloading final checkpoint...\")\n",
    "        files.download(os.path.join(CHECKPOINT_DIR, \"final.pt\"))\n",
    "except ImportError:\n",
    "    print(\"\\nNot running on Colab — skipping download.\")\n",
    "    print(f\"Checkpoints are at: {os.path.abspath(CHECKPOINT_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Bonus: Load Checkpoint & Generate (proves the save works)\n",
    "\n",
    "# Create a fresh model from config\n",
    "loaded_config = ModelConfig.load(os.path.join(CHECKPOINT_DIR, \"model_config.json\"))\n",
    "loaded_model = LLaMA(loaded_config).to(device)\n",
    "\n",
    "# Load the best checkpoint\n",
    "ckpt_path = os.path.join(CHECKPOINT_DIR, \"best.pt\")\n",
    "if not os.path.exists(ckpt_path):\n",
    "    ckpt_path = os.path.join(CHECKPOINT_DIR, \"final.pt\")\n",
    "\n",
    "info = load_checkpoint(ckpt_path, loaded_model, device=device)\n",
    "print(f\"Loaded checkpoint from step {info['step']}, val_loss {info['val_loss']:.4f}\")\n",
    "\n",
    "# Generate with loaded model\n",
    "print(\"\\n--- Generation from loaded checkpoint ---\")\n",
    "for prompt in [\"Once upon a time\", \"The little cat was\"]:\n",
    "    text = generate(\n",
    "        loaded_model, tokenizer, prompt,\n",
    "        max_new_tokens=100, temperature=0.8, device=device,\n",
    "    )\n",
    "    print(f\"\\nPrompt: \\\"{prompt}\\\"\")\n",
    "    print(text)\n",
    "\n",
    "print(\"\\nCheckpoint load + generate: SUCCESS\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}