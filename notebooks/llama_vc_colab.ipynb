{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA from Scratch: Train & Generate on Google Colab\n",
    "\n",
    "This notebook trains a **~15M parameter LLaMA model** from scratch in pure PyTorch, end-to-end:\n",
    "\n",
    "1. **Choose a dataset** (TinyStories, GSM8K, SimpleMath, AQUA-RAT, or mixed)\n",
    "2. **Train a BPE tokenizer** (SentencePiece, covering all selected datasets)\n",
    "3. **Tokenize the dataset** into memory-mapped binary files\n",
    "4. **Train the model** with mixed precision, gradient accumulation, cosine LR schedule\n",
    "5. **Evaluate** on the validation set (loss + perplexity)\n",
    "6. **Generate text** from prompts with temperature/top-k/top-p sampling\n",
    "7. **Save & download** the trained checkpoint\n",
    "\n",
    "**Datasets:**\n",
    "| Name | Description | Size |\n",
    "|------|-------------|------|\n",
    "| `tinystories` | Short children's stories (default) | ~2.1M examples |\n",
    "| `gsm8k` | Grade school math word problems | 8.5K examples |\n",
    "| `simplemath` | Basic arithmetic problems | 100K examples |\n",
    "| `aqua_rat` | Word problems with reasoning | 98K examples |\n",
    "| `mixed` | All of the above combined | — |\n",
    "\n",
    "**Architecture:** Decoder-only transformer following LLaMA (Meta AI)\n",
    "- RMSNorm (pre-normalization)\n",
    "- Rotary Positional Embeddings (RoPE)\n",
    "- SwiGLU activation in FFN\n",
    "- Grouped Query Attention (GQA, 6 query heads / 2 KV heads)\n",
    "- KV cache for efficient inference\n",
    "\n",
    "**Requirements:** A Colab GPU runtime (T4 or A100). Go to *Runtime > Change runtime type > GPU*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup: Install Dependencies & Clone Repo\n",
    "!pip install -q torch sentencepiece datasets tqdm\n",
    "!git clone -q https://github.com/manojkgorle/smol-llama2.git\n",
    "%cd smol-llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Configuration — Edit these parameters!\n",
    "\n",
    "# ── Dataset Selection ────────────────────────────────────────────────────\n",
    "# Choose which dataset to train on:\n",
    "#   \"tinystories\"  — ~2.1M short stories (default)\n",
    "#   \"gsm8k\"        — 8.5K grade school math word problems\n",
    "#   \"simplemath\"   — 100K basic arithmetic problems\n",
    "#   \"aqua_rat\"     — 98K word problems with reasoning\n",
    "#   \"mixed\"        — All of the above combined\n",
    "DATASET = \"tinystories\"\n",
    "\n",
    "# Which datasets to include in tokenizer training.\n",
    "# The tokenizer should cover ALL text the model will ever see.\n",
    "# Use [\"tinystories\"] for stories only, or list all datasets you plan to use.\n",
    "# Ignored if DATASET != \"mixed\" (auto-set to just [DATASET] for single datasets).\n",
    "TOKENIZER_DATASETS = [\"tinystories\", \"gsm8k\", \"simplemath\", \"aqua_rat\"]\n",
    "\n",
    "# ── Training ─────────────────────────────────────────────────────────────\n",
    "TRAINING_STEPS = 3000          # Total optimizer steps (~15 min on T4)\n",
    "BATCH_SIZE = 64                # Sequences per micro-batch\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Effective batch = 64 * 4 = 256 sequences\n",
    "LEARNING_RATE = 3e-4           # Peak LR (after warmup)\n",
    "MIN_LEARNING_RATE = 3e-5       # Floor LR (10% of peak)\n",
    "WARMUP_STEPS = 200             # Linear warmup steps\n",
    "WEIGHT_DECAY = 0.1             # AdamW weight decay\n",
    "MAX_GRAD_NORM = 1.0            # Gradient clipping\n",
    "\n",
    "# ── Evaluation & Logging ─────────────────────────────────────────────────\n",
    "EVAL_INTERVAL = 500            # Evaluate every N steps\n",
    "EVAL_STEPS = 20                # Batches per evaluation\n",
    "LOG_INTERVAL = 50              # Print loss every N steps\n",
    "SAVE_INTERVAL = 1000           # Save checkpoint every N steps\n",
    "\n",
    "# ── Model Architecture ──────────────────────────────────────────────────\n",
    "VOCAB_SIZE = 4096\n",
    "DIM = 384\n",
    "N_LAYERS = 8\n",
    "N_HEADS = 6\n",
    "N_KV_HEADS = 2\n",
    "MAX_SEQ_LEN = 512\n",
    "HIDDEN_DIM = 1024\n",
    "\n",
    "# ── Paths ────────────────────────────────────────────────────────────────\n",
    "DATA_DIR = \"data/\"\n",
    "CHECKPOINT_DIR = \"checkpoints/\"\n",
    "TOKENIZER_PREFIX = \"data/tokenizer\"  # produces data/tokenizer.model\n",
    "SEED = 42\n",
    "\n",
    "# ── Auto-configure tokenizer datasets for single-dataset mode ────────────\n",
    "if DATASET != \"mixed\":\n",
    "    TOKENIZER_DATASETS = [DATASET]\n",
    "\n",
    "print(f\"Dataset: {DATASET}\")\n",
    "print(f\"Tokenizer trained on: {TOKENIZER_DATASETS}\")\n",
    "print(f\"Training for {TRAINING_STEPS} steps\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS} sequences\")\n",
    "print(f\"Tokens per step: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS * MAX_SEQ_LEN:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Device Detection\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    props = torch.cuda.get_device_properties(device)\n",
    "    print(f\"GPU: {props.name}\")\n",
    "    print(f\"VRAM: {props.total_mem / 1024**3:.1f} GB\")\n",
    "    print(f\"Compute Capability: {props.major}.{props.minor}\")\n",
    "    print(f\"BF16 Support: {torch.cuda.is_bf16_supported()}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Device: MPS (Apple Silicon)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"WARNING: No GPU detected! Training will be very slow.\")\n",
    "    print(\"Go to Runtime > Change runtime type > GPU\")\n",
    "\n",
    "print(f\"\\nDevice: {device}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import Modules\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from llama_vc.config import ModelConfig\n",
    "from llama_vc.model import LLaMA\n",
    "from llama_vc.tokenizer import Tokenizer, train_tokenizer\n",
    "from llama_vc.dataset import (\n",
    "    download_dataset, prepare_dataset, prepare_mixed,\n",
    "    create_dataloader, DATASETS, DATASET_NAMES,\n",
    ")\n",
    "from llama_vc.device import (\n",
    "    get_dtype, get_autocast_context,\n",
    "    get_grad_scaler, get_memory_usage,\n",
    ")\n",
    "from llama_vc.generate import generate\n",
    "from llama_vc.train import get_lr, evaluate\n",
    "from llama_vc.utils import (\n",
    "    set_seed, count_parameters, print_model_summary,\n",
    "    save_checkpoint, load_checkpoint,\n",
    ")\n",
    "\n",
    "print(\"All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Train Tokenizer\n",
    "\n",
    "We train a **BPE tokenizer** (Byte-Pair Encoding) with SentencePiece on text from all selected datasets (`TOKENIZER_DATASETS`).\n",
    "\n",
    "- **Vocab size:** 4096 tokens (small, matching our tiny model)\n",
    "- **Byte fallback:** Unknown characters are encoded as UTF-8 bytes (no `<unk>` tokens)\n",
    "- **Special tokens:** `<s>` (BOS, id=1), `</s>` (EOS, id=2)\n",
    "- **Multi-dataset:** SentencePiece accepts comma-separated input files, so the vocabulary covers all selected text sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 1: Train Tokenizer\n",
    "\n",
    "# Download text files for each dataset the tokenizer needs to cover\n",
    "text_files = []\n",
    "for ds_name in TOKENIZER_DATASETS:\n",
    "    print(f\"--- Downloading {ds_name} ---\")\n",
    "    train_txt, _ = download_dataset(ds_name, DATA_DIR)\n",
    "    text_files.append(train_txt)\n",
    "\n",
    "# Train the tokenizer on all text sources\n",
    "# SentencePiece natively accepts comma-separated input files\n",
    "tokenizer_model_path = TOKENIZER_PREFIX + \".model\"\n",
    "if os.path.exists(tokenizer_model_path):\n",
    "    print(f\"\\nTokenizer already exists: {tokenizer_model_path}\")\n",
    "else:\n",
    "    combined_input = \",\".join(text_files)\n",
    "    print(f\"\\nTraining tokenizer on {len(text_files)} source(s):\")\n",
    "    for f in text_files:\n",
    "        size_mb = os.path.getsize(f) / 1024**2\n",
    "        print(f\"  {f} ({size_mb:.1f} MB)\")\n",
    "    train_tokenizer(\n",
    "        input_file=combined_input,\n",
    "        model_prefix=TOKENIZER_PREFIX,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "    )\n",
    "\n",
    "# Load and verify\n",
    "tokenizer = Tokenizer(tokenizer_model_path)\n",
    "print(f\"\\nTokenizer loaded: vocab_size={tokenizer.vocab_size}\")\n",
    "\n",
    "# Test roundtrip on story + math text\n",
    "test_texts = [\n",
    "    \"Once upon a time, there was a little cat.\",\n",
    "    \"Question: What is 48 / 2?\\nAnswer: 24\",\n",
    "]\n",
    "for text in test_texts:\n",
    "    tokens = tokenizer.encode(text, bos=False, eos=False)\n",
    "    decoded = tokenizer.decode(tokens)\n",
    "    status = \"PASS\" if decoded == text else \"FAIL\"\n",
    "    print(f\"[{status}] Roundtrip: '{text[:50]}...' ({len(tokens)} tokens)\"\n",
    "          if len(text) > 50 else\n",
    "          f\"[{status}] Roundtrip: '{text}' ({len(tokens)} tokens)\")\n",
    "print(\"Tokenizer ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 2: Prepare Data (Tokenize to Binary)\n",
    "\n",
    "# Prepare the selected dataset (download + tokenize to .bin)\n",
    "if DATASET == \"mixed\":\n",
    "    train_bin, val_bin = prepare_mixed(DATA_DIR, tokenizer)\n",
    "else:\n",
    "    train_bin, val_bin = prepare_dataset(DATASET, DATA_DIR, tokenizer)\n",
    "\n",
    "# Validate — remove empty/corrupt files so re-run will regenerate them\n",
    "for bin_path in [train_bin, val_bin]:\n",
    "    if os.path.exists(bin_path) and os.path.getsize(bin_path) == 0:\n",
    "        print(f\"WARNING: {bin_path} is empty, removing so it will be regenerated.\")\n",
    "        os.remove(bin_path)\n",
    "\n",
    "# Re-run if needed\n",
    "if not os.path.exists(train_bin) or not os.path.exists(val_bin):\n",
    "    if DATASET == \"mixed\":\n",
    "        train_bin, val_bin = prepare_mixed(DATA_DIR, tokenizer)\n",
    "    else:\n",
    "        train_bin, val_bin = prepare_dataset(DATASET, DATA_DIR, tokenizer)\n",
    "\n",
    "# Print dataset stats\n",
    "train_tokens = os.path.getsize(train_bin) // 2  # uint16 = 2 bytes\n",
    "val_tokens = os.path.getsize(val_bin) // 2\n",
    "print(f\"\\nDataset: {DATASET}\")\n",
    "print(f\"Train tokens: {train_tokens:,}\")\n",
    "print(f\"Val tokens:   {val_tokens:,}\")\n",
    "print(f\"Train file:   {os.path.getsize(train_bin) / 1024**2:.1f} MB\")\n",
    "print(f\"Val file:     {os.path.getsize(val_bin) / 1024**2:.1f} MB\")\n",
    "\n",
    "assert train_tokens > 0, \"train.bin is empty — data preparation failed!\"\n",
    "assert val_tokens > 0, \"val.bin is empty — data preparation failed!\"\n",
    "print(\"Data validation: PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 3: Create Model\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# Build model config (update vocab_size from tokenizer)\n",
    "model_config = ModelConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    dim=DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    n_kv_heads=N_KV_HEADS,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    ")\n",
    "model_config.validate()\n",
    "\n",
    "# Create model\n",
    "model = LLaMA(model_config).to(device)\n",
    "n_params = count_parameters(model)\n",
    "print(f\"\\nModel parameters: {n_params:,}\")\n",
    "print_model_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train\n",
    "\n",
    "The training loop below runs for `TRAINING_STEPS` optimizer steps with:\n",
    "- **Mixed precision** (bf16 on Ampere+, fp16 on T4, fp32 on CPU)\n",
    "- **Gradient accumulation** (effective batch = `BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS`)\n",
    "- **Cosine LR schedule** with linear warmup\n",
    "- **Gradient clipping** to prevent explosions\n",
    "- **Periodic evaluation** on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 4: Train the Model\n",
    "\n",
    "# ── Setup ────────────────────────────────────────────────────────────────\n",
    "dtype = get_dtype(\"auto\", device)\n",
    "autocast_ctx = get_autocast_context(device, dtype)\n",
    "scaler = get_grad_scaler(device, dtype)\n",
    "print(f\"Training dtype: {dtype}\")\n",
    "print(f\"GradScaler: {'enabled' if scaler else 'disabled'}\")\n",
    "\n",
    "# ── DataLoaders ──────────────────────────────────────────────────────────\n",
    "train_loader = create_dataloader(\n",
    "    train_bin, seq_len=MAX_SEQ_LEN, batch_size=BATCH_SIZE,\n",
    "    shuffle=True, pin_memory=(device.type != \"cpu\"),\n",
    ")\n",
    "val_loader = create_dataloader(\n",
    "    val_bin, seq_len=MAX_SEQ_LEN, batch_size=BATCH_SIZE,\n",
    "    shuffle=False, pin_memory=(device.type != \"cpu\"),\n",
    ")\n",
    "\n",
    "# ── Optimizer ────────────────────────────────────────────────────────────\n",
    "optimizer = model.configure_optimizers(\n",
    "    learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY,\n",
    "    betas=(0.9, 0.95), device=device,\n",
    ")\n",
    "\n",
    "# ── Training Loop ────────────────────────────────────────────────────────\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "model.train()\n",
    "train_iter = iter(train_loader)\n",
    "tokens_per_step = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS * MAX_SEQ_LEN\n",
    "best_val_loss = float(\"inf\")\n",
    "train_losses = []\n",
    "\n",
    "print(f\"\\nStarting training: {TRAINING_STEPS} steps\")\n",
    "print(f\"Effective batch: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS} sequences = {tokens_per_step:,} tokens/step\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "pbar = tqdm(range(TRAINING_STEPS), desc=\"Training\", unit=\"step\")\n",
    "for step in pbar:\n",
    "    step_start = time.perf_counter()\n",
    "\n",
    "    # ── Learning Rate Schedule ───────────────────────────────────────────\n",
    "    lr = get_lr(step, WARMUP_STEPS, TRAINING_STEPS, LEARNING_RATE, MIN_LEARNING_RATE)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "    # ── Gradient Accumulation ────────────────────────────────────────────\n",
    "    accumulated_loss = 0.0\n",
    "    for micro_step in range(GRADIENT_ACCUMULATION_STEPS):\n",
    "        try:\n",
    "            x, y = next(train_iter)\n",
    "        except StopIteration:\n",
    "            train_iter = iter(train_loader)\n",
    "            x, y = next(train_iter)\n",
    "\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        with autocast_ctx:\n",
    "            _, loss = model(x, targets=y)\n",
    "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        accumulated_loss += loss.item()\n",
    "\n",
    "    # ── Gradient Clipping + Optimizer Step ───────────────────────────────\n",
    "    if scaler is not None:\n",
    "        scaler.unscale_(optimizer)\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "\n",
    "    if scaler is not None:\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    else:\n",
    "        optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # ── Timing ───────────────────────────────────────────────────────────\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    step_time = time.perf_counter() - step_start\n",
    "    tok_per_sec = tokens_per_step / step_time\n",
    "\n",
    "    # ── Progress Bar ─────────────────────────────────────────────────────\n",
    "    train_losses.append(accumulated_loss)\n",
    "    pbar.set_postfix(loss=f\"{accumulated_loss:.4f}\", lr=f\"{lr:.2e}\", tps=f\"{tok_per_sec:,.0f}\")\n",
    "\n",
    "    # ── Log ──────────────────────────────────────────────────────────────\n",
    "    if step % LOG_INTERVAL == 0:\n",
    "        mem = get_memory_usage(device)\n",
    "        print(\n",
    "            f\"step {step:>5d}/{TRAINING_STEPS} | \"\n",
    "            f\"loss {accumulated_loss:.4f} | \"\n",
    "            f\"lr {lr:.2e} | \"\n",
    "            f\"{tok_per_sec:>8,.0f} tok/s | \"\n",
    "            f\"mem {mem['allocated_mb']:>6.0f} MB\"\n",
    "        )\n",
    "\n",
    "    # ── Evaluation ───────────────────────────────────────────────────────\n",
    "    if step > 0 and step % EVAL_INTERVAL == 0:\n",
    "        val_loss = evaluate(model, val_loader, device, autocast_ctx, max_steps=EVAL_STEPS)\n",
    "        perplexity = math.exp(val_loss)\n",
    "        print(f\"{'─' * 60}\")\n",
    "        print(f\"EVAL step {step} | val_loss {val_loss:.4f} | perplexity {perplexity:.2f}\")\n",
    "        print(f\"{'─' * 60}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_checkpoint(\n",
    "                model, optimizer, step, val_loss,\n",
    "                model_config.to_dict(), {},\n",
    "                os.path.join(CHECKPOINT_DIR, \"best.pt\"),\n",
    "            )\n",
    "        model.train()\n",
    "\n",
    "    # ── Periodic Checkpoint ──────────────────────────────────────────────\n",
    "    if step > 0 and step % SAVE_INTERVAL == 0:\n",
    "        save_checkpoint(\n",
    "            model, optimizer, step, best_val_loss,\n",
    "            model_config.to_dict(), {},\n",
    "            os.path.join(CHECKPOINT_DIR, f\"step_{step:06d}.pt\"),\n",
    "        )\n",
    "\n",
    "# ── Final evaluation + checkpoint ────────────────────────────────────────\n",
    "print(\"\\nTraining complete! Running final evaluation...\")\n",
    "val_loss = evaluate(model, val_loader, device, autocast_ctx, max_steps=EVAL_STEPS)\n",
    "perplexity = math.exp(val_loss)\n",
    "print(f\"Final val_loss: {val_loss:.4f} | perplexity: {perplexity:.2f}\")\n",
    "\n",
    "if val_loss < best_val_loss:\n",
    "    best_val_loss = val_loss\n",
    "    save_checkpoint(\n",
    "        model, optimizer, TRAINING_STEPS, val_loss,\n",
    "        model_config.to_dict(), {},\n",
    "        os.path.join(CHECKPOINT_DIR, \"best.pt\"),\n",
    "    )\n",
    "    print(f\"New best checkpoint saved at step {TRAINING_STEPS}!\")\n",
    "\n",
    "final_path = os.path.join(CHECKPOINT_DIR, \"final.pt\")\n",
    "save_checkpoint(\n",
    "    model, optimizer, TRAINING_STEPS, best_val_loss,\n",
    "    model_config.to_dict(), {}, final_path,\n",
    ")\n",
    "print(f\"Final checkpoint saved: {final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 5: Evaluate\n",
    "\n",
    "val_loss = evaluate(model, val_loader, device, autocast_ctx, max_steps=EVAL_STEPS)\n",
    "perplexity = math.exp(val_loss)\n",
    "\n",
    "print(f\"Final Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Final Perplexity:      {perplexity:.2f}\")\n",
    "print(f\"Best Validation Loss:  {best_val_loss:.4f}\")\n",
    "print(f\"Best Perplexity:       {math.exp(best_val_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 6: Generate Text\n",
    "\n",
    "# Dataset-appropriate prompts\n",
    "PROMPTS = {\n",
    "    \"tinystories\": [\n",
    "        \"Once upon a time\",\n",
    "        \"The little dog\",\n",
    "        \"She looked at the sky and\",\n",
    "        \"One day, a boy named Tom\",\n",
    "    ],\n",
    "    \"gsm8k\": [\n",
    "        \"Question: Sarah has 24 apples and gives half to her friend. How many does she have left?\\nAnswer:\",\n",
    "        \"Question: A train travels 60 miles per hour for 3 hours. How far does it go?\\nAnswer:\",\n",
    "    ],\n",
    "    \"simplemath\": [\n",
    "        \"245 + 378\\nAnswer:\",\n",
    "        \"1000 - 457\\nAnswer:\",\n",
    "    ],\n",
    "    \"aqua_rat\": [\n",
    "        \"Question: If a shirt costs $40 and is on sale for 25% off, what is the sale price?\\nOptions:\",\n",
    "        \"Question: A car travels 180 miles in 3 hours. What is its average speed?\\nOptions:\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Use matching prompts, or story prompts for mixed\n",
    "prompts = PROMPTS.get(DATASET, PROMPTS[\"tinystories\"])\n",
    "\n",
    "for temp in [0.7, 1.0]:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Temperature = {temp}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    for prompt in prompts:\n",
    "        result = generate(\n",
    "            model, tokenizer, prompt,\n",
    "            max_new_tokens=150, temperature=temp,\n",
    "            top_k=40, top_p=0.9, device=device,\n",
    "        )\n",
    "        print(f\"\\n--- Prompt: \\\"{prompt[:60]}{'...' if len(prompt) > 60 else ''}\\\" ---\")\n",
    "        print(result.text)\n",
    "        print(result.stats_string())\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 7: Save & Download Model\n",
    "\n",
    "import zipfile\n",
    "\n",
    "# Save model config alongside checkpoint\n",
    "config_path = os.path.join(CHECKPOINT_DIR, \"model_config.json\")\n",
    "model_config.save(config_path)\n",
    "print(f\"Model config saved to {config_path}\")\n",
    "\n",
    "# Determine which checkpoint to use as primary\n",
    "best_path = os.path.join(CHECKPOINT_DIR, \"best.pt\")\n",
    "final_path = os.path.join(CHECKPOINT_DIR, \"final.pt\")\n",
    "primary_ckpt = best_path if os.path.exists(best_path) else final_path\n",
    "\n",
    "# List all checkpoints\n",
    "print(\"\\nCheckpoints:\")\n",
    "for f in sorted(os.listdir(CHECKPOINT_DIR)):\n",
    "    path = os.path.join(CHECKPOINT_DIR, f)\n",
    "    size_mb = os.path.getsize(path) / 1024**2\n",
    "    print(f\"  {f}: {size_mb:.1f} MB\")\n",
    "\n",
    "# Bundle all files needed for inference into a single zip:\n",
    "#   - Model weights (best.pt or final.pt)\n",
    "#   - Tokenizer model (tokenizer.model)\n",
    "#   - Model config (model_config.json)\n",
    "tokenizer_path = TOKENIZER_PREFIX + \".model\"\n",
    "\n",
    "artifacts = {\n",
    "    \"checkpoint\": primary_ckpt,\n",
    "    \"tokenizer\": tokenizer_path,\n",
    "    \"config\": config_path,\n",
    "}\n",
    "\n",
    "# Verify all files exist\n",
    "print(\"\\nInference artifacts:\")\n",
    "for name, path in artifacts.items():\n",
    "    exists = os.path.exists(path)\n",
    "    size_mb = os.path.getsize(path) / 1024**2 if exists else 0\n",
    "    status = f\"{size_mb:.1f} MB\" if exists else \"MISSING\"\n",
    "    print(f\"  {name:12s}: {path} ({status})\")\n",
    "\n",
    "zip_path = \"llama_vc_model.zip\"\n",
    "with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
    "    for name, path in artifacts.items():\n",
    "        if os.path.exists(path):\n",
    "            zf.write(path, os.path.basename(path))\n",
    "\n",
    "zip_size_mb = os.path.getsize(zip_path) / 1024**2\n",
    "print(f\"\\nBundled into: {zip_path} ({zip_size_mb:.1f} MB)\")\n",
    "\n",
    "# Download (Colab only)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"Downloading zip...\")\n",
    "    files.download(zip_path)\n",
    "except ImportError:\n",
    "    print(\"Not running on Colab — skipping download.\")\n",
    "    print(f\"Zip is at: {os.path.abspath(zip_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Bonus: Load Checkpoint & Generate (proves the save works)\n",
    "\n",
    "# Create a fresh model from config\n",
    "loaded_config = ModelConfig.load(os.path.join(CHECKPOINT_DIR, \"model_config.json\"))\n",
    "loaded_model = LLaMA(loaded_config).to(device)\n",
    "\n",
    "# Load the best checkpoint\n",
    "ckpt_path = os.path.join(CHECKPOINT_DIR, \"best.pt\")\n",
    "if not os.path.exists(ckpt_path):\n",
    "    ckpt_path = os.path.join(CHECKPOINT_DIR, \"final.pt\")\n",
    "\n",
    "info = load_checkpoint(ckpt_path, loaded_model, device=device)\n",
    "print(f\"Loaded checkpoint from step {info['step']}, val_loss {info['val_loss']:.4f}\")\n",
    "\n",
    "# Generate with loaded model\n",
    "print(\"\\n--- Generation from loaded checkpoint ---\")\n",
    "for prompt in [\"Once upon a time\", \"The little cat was\"]:\n",
    "    result = generate(\n",
    "        loaded_model, tokenizer, prompt,\n",
    "        max_new_tokens=100, temperature=0.8, device=device,\n",
    "    )\n",
    "    print(f\"\\nPrompt: \\\"{prompt}\\\"\")\n",
    "    print(result.text)\n",
    "    print(result.stats_string())\n",
    "\n",
    "print(\"\\nCheckpoint load + generate: SUCCESS\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
