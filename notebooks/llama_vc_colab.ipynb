{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA from Scratch: Train & Generate on Google Colab\n",
    "\n",
    "This notebook trains a **~15M parameter LLaMA model** on the [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset, end-to-end:\n",
    "\n",
    "1. **Train a BPE tokenizer** (SentencePiece, 4096 vocab)\n",
    "2. **Tokenize the dataset** into memory-mapped binary files\n",
    "3. **Train the model** with mixed precision, gradient accumulation, cosine LR schedule\n",
    "4. **Evaluate** on the validation set (loss + perplexity)\n",
    "5. **Generate text** from prompts with temperature/top-k/top-p sampling\n",
    "6. **Save & download** the trained checkpoint\n",
    "\n",
    "**Architecture:** Decoder-only transformer following LLaMA (Meta AI)\n",
    "- RMSNorm (pre-normalization)\n",
    "- Rotary Positional Embeddings (RoPE)\n",
    "- SwiGLU activation in FFN\n",
    "- Grouped Query Attention (GQA, 6 query heads / 2 KV heads)\n",
    "- KV cache for efficient inference\n",
    "\n",
    "**Requirements:** A Colab GPU runtime (T4 or A100). Go to *Runtime > Change runtime type > GPU*.\n",
    "\n",
    "The entire LLaMA implementation is self-contained in this notebook — no external package needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install Dependencies\n",
    "!pip install -q torch sentencepiece datasets tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Configuration — Edit these parameters!\n",
    "\n",
    "# ── Training ─────────────────────────────────────────────────────────────\n",
    "TRAINING_STEPS = 3000          # Total optimizer steps (~15 min on T4)\n",
    "BATCH_SIZE = 64                # Sequences per micro-batch\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Effective batch = 64 * 4 = 256 sequences\n",
    "LEARNING_RATE = 3e-4           # Peak LR (after warmup)\n",
    "MIN_LEARNING_RATE = 3e-5       # Floor LR (10% of peak)\n",
    "WARMUP_STEPS = 200             # Linear warmup steps\n",
    "WEIGHT_DECAY = 0.1             # AdamW weight decay\n",
    "MAX_GRAD_NORM = 1.0            # Gradient clipping\n",
    "\n",
    "# ── Evaluation & Logging ─────────────────────────────────────────────────\n",
    "EVAL_INTERVAL = 500            # Evaluate every N steps\n",
    "EVAL_STEPS = 20                # Batches per evaluation\n",
    "LOG_INTERVAL = 50              # Print loss every N steps\n",
    "SAVE_INTERVAL = 1000           # Save checkpoint every N steps\n",
    "\n",
    "# ── Model Architecture ──────────────────────────────────────────────────\n",
    "VOCAB_SIZE = 4096\n",
    "DIM = 384\n",
    "N_LAYERS = 8\n",
    "N_HEADS = 6\n",
    "N_KV_HEADS = 2\n",
    "MAX_SEQ_LEN = 512\n",
    "HIDDEN_DIM = 1024\n",
    "\n",
    "# ── Paths ────────────────────────────────────────────────────────────────\n",
    "DATA_DIR = \"data/\"\n",
    "CHECKPOINT_DIR = \"checkpoints/\"\n",
    "TOKENIZER_PREFIX = \"data/tokenizer\"  # produces data/tokenizer.model\n",
    "SEED = 42\n",
    "\n",
    "print(f\"Training for {TRAINING_STEPS} steps\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS} sequences\")\n",
    "print(f\"Tokens per step: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS * MAX_SEQ_LEN:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Device Detection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.amp\n",
    "from torch.utils.checkpoint import checkpoint as gradient_checkpoint\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    props = torch.cuda.get_device_properties(device)\n",
    "    print(f\"GPU: {props.name}\")\n",
    "    print(f\"VRAM: {props.total_mem / 1024**3:.1f} GB\")\n",
    "    print(f\"Compute Capability: {props.major}.{props.minor}\")\n",
    "    print(f\"BF16 Support: {torch.cuda.is_bf16_supported()}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Device: MPS (Apple Silicon)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"WARNING: No GPU detected! Training will be very slow.\")\n",
    "    print(\"Go to Runtime > Change runtime type > GPU\")\n",
    "\n",
    "print(f\"\\nDevice: {device}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Implementation\n",
    "\n",
    "The next few cells define the full LLaMA implementation inline:\n",
    "- **Config** — `ModelConfig` and `TrainConfig` dataclasses\n",
    "- **Model** — RMSNorm, RoPE, SwiGLU FFN, GQA Attention, Transformer blocks\n",
    "- **Tokenizer** — SentencePiece BPE wrapper\n",
    "- **Data Pipeline** — Download, tokenize, DataLoader\n",
    "- **Utilities** — Device helpers, checkpointing, generation, training helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Config Classes\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional, Tuple\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Architecture hyperparameters for the LLaMA model.\"\"\"\n",
    "    vocab_size: int = 4096\n",
    "    dim: int = 384\n",
    "    n_layers: int = 8\n",
    "    n_heads: int = 6\n",
    "    n_kv_heads: int = 2\n",
    "    max_seq_len: int = 512\n",
    "    hidden_dim: int = 1024\n",
    "    norm_eps: float = 1e-5\n",
    "    rope_theta: float = 10000.0\n",
    "    dropout: float = 0.0\n",
    "    use_gradient_checkpointing: bool = False\n",
    "    weight_tying: bool = False\n",
    "\n",
    "    @property\n",
    "    def head_dim(self) -> int:\n",
    "        assert self.dim % self.n_heads == 0\n",
    "        return self.dim // self.n_heads\n",
    "\n",
    "    @property\n",
    "    def n_kv_groups(self) -> int:\n",
    "        assert self.n_heads % self.n_kv_heads == 0\n",
    "        return self.n_heads // self.n_kv_heads\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        assert self.dim > 0 and self.n_layers > 0\n",
    "        assert self.n_heads > 0 and self.n_kv_heads > 0\n",
    "        assert self.n_kv_heads <= self.n_heads\n",
    "        assert self.dim % self.n_heads == 0\n",
    "        assert self.n_heads % self.n_kv_heads == 0\n",
    "        assert self.head_dim % 2 == 0\n",
    "        assert self.vocab_size > 0 and self.max_seq_len > 0\n",
    "        assert self.hidden_dim > 0\n",
    "        assert 0.0 <= self.dropout < 1.0\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        return asdict(self)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, d: dict) -> \"ModelConfig\":\n",
    "        return cls(**d)\n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(self.to_dict(), f, indent=2)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> \"ModelConfig\":\n",
    "        with open(path, \"r\") as f:\n",
    "            return cls.from_dict(json.load(f))\n",
    "\n",
    "\n",
    "print(\"Config classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Model Architecture (RMSNorm, RoPE, SwiGLU, GQA, LLaMA)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization.\"\"\"\n",
    "    def __init__(self, dim: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        rms_inv = torch.rsqrt(x.float().pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return (x.float() * rms_inv).type_as(x) * self.weight\n",
    "\n",
    "\n",
    "def precompute_rope_frequencies(\n",
    "    head_dim: int, max_seq_len: int, theta: float = 10000.0,\n",
    "    device: Optional[torch.device] = None,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Precompute cos/sin tables for Rotary Positional Embeddings.\"\"\"\n",
    "    assert head_dim % 2 == 0\n",
    "    dim_indices = torch.arange(0, head_dim, 2, device=device).float()\n",
    "    freqs = 1.0 / (theta ** (dim_indices / head_dim))\n",
    "    positions = torch.arange(max_seq_len, device=device).float()\n",
    "    angles = torch.outer(positions, freqs)\n",
    "    return angles.cos(), angles.sin()\n",
    "\n",
    "\n",
    "def apply_rotary_embeddings(\n",
    "    x: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Apply RoPE rotation to query or key tensors.\"\"\"\n",
    "    x_reshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "    x_even = x_reshaped[..., 0]\n",
    "    x_odd = x_reshaped[..., 1]\n",
    "    cos = freqs_cos.unsqueeze(0).unsqueeze(2)\n",
    "    sin = freqs_sin.unsqueeze(0).unsqueeze(2)\n",
    "    x_even_rot = x_even * cos - x_odd * sin\n",
    "    x_odd_rot = x_even * sin + x_odd * cos\n",
    "    x_rotated = torch.stack([x_even_rot, x_odd_rot], dim=-1).flatten(-2)\n",
    "    return x_rotated.type_as(x)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"SwiGLU Feed-Forward Network.\"\"\"\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.w_gate = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
    "        self.w_up = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
    "        self.w_down = nn.Linear(config.hidden_dim, config.dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.w_down(F.silu(self.w_gate(x)) * self.w_up(x))\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Grouped Query Attention with KV Cache.\"\"\"\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.n_heads = config.n_heads\n",
    "        self.n_kv_heads = config.n_kv_heads\n",
    "        self.head_dim = config.head_dim\n",
    "        self.n_kv_groups = config.n_kv_groups\n",
    "        self.wq = nn.Linear(config.dim, config.n_heads * config.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(config.dim, config.n_kv_heads * config.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(config.dim, config.n_kv_heads * config.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(config.n_heads * config.head_dim, config.dim, bias=False)\n",
    "\n",
    "    def forward(self, x, freqs_cos, freqs_sin, mask=None, kv_cache=None, start_pos=0):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        q = self.wq(x).view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        k = self.wk(x).view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        v = self.wv(x).view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        q = apply_rotary_embeddings(q, freqs_cos, freqs_sin)\n",
    "        k = apply_rotary_embeddings(k, freqs_cos, freqs_sin)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            cached_k, cached_v = kv_cache\n",
    "            k = torch.cat([cached_k, k], dim=1)\n",
    "            v = torch.cat([cached_v, v], dim=1)\n",
    "        new_kv_cache = (k, v)\n",
    "\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        if self.n_kv_heads != self.n_heads:\n",
    "            k = k.repeat_interleave(self.n_kv_groups, dim=1)\n",
    "            v = v.repeat_interleave(self.n_kv_groups, dim=1)\n",
    "\n",
    "        if mask is not None:\n",
    "            output = F.scaled_dot_product_attention(q, k, v, attn_mask=mask)\n",
    "        else:\n",
    "            output = F.scaled_dot_product_attention(q, k, v, is_causal=(seq_len > 1))\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        return self.wo(output), new_kv_cache\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"One LLaMA transformer decoder layer.\"\"\"\n",
    "    def __init__(self, layer_id: int, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(config.dim, config.norm_eps)\n",
    "        self.attention = Attention(config)\n",
    "        self.ffn_norm = RMSNorm(config.dim, config.norm_eps)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "    def forward(self, x, freqs_cos, freqs_sin, mask=None, kv_cache=None, start_pos=0):\n",
    "        attn_output, new_kv_cache = self.attention(\n",
    "            self.attention_norm(x), freqs_cos, freqs_sin, mask, kv_cache, start_pos\n",
    "        )\n",
    "        x = x + attn_output\n",
    "        x = x + self.feed_forward(self.ffn_norm(x))\n",
    "        return x, new_kv_cache\n",
    "\n",
    "\n",
    "class LLaMA(nn.Module):\n",
    "    \"\"\"Complete LLaMA decoder-only transformer language model.\"\"\"\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        config.validate()\n",
    "        self.config = config\n",
    "        self.tok_embeddings = nn.Embedding(config.vocab_size, config.dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(layer_id=i, config=config)\n",
    "            for i in range(config.n_layers)\n",
    "        ])\n",
    "        self.norm = RMSNorm(config.dim, config.norm_eps)\n",
    "        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)\n",
    "\n",
    "        if config.weight_tying:\n",
    "            self.output.weight = self.tok_embeddings.weight\n",
    "\n",
    "        freqs_cos, freqs_sin = precompute_rope_frequencies(\n",
    "            config.head_dim, config.max_seq_len, config.rope_theta\n",
    "        )\n",
    "        self.register_buffer(\"freqs_cos\", freqs_cos, persistent=False)\n",
    "        self.register_buffer(\"freqs_sin\", freqs_sin, persistent=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for layer in self.layers:\n",
    "            scale = 1.0 / math.sqrt(2 * config.n_layers)\n",
    "            nn.init.normal_(layer.attention.wo.weight, mean=0.0, std=0.02 * scale)\n",
    "            nn.init.normal_(layer.feed_forward.w_down.weight, mean=0.0, std=0.02 * scale)\n",
    "\n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, tokens, targets=None, kv_caches=None, start_pos=0):\n",
    "        batch_size, seq_len = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        freqs_cos = self.freqs_cos[start_pos: start_pos + seq_len]\n",
    "        freqs_sin = self.freqs_sin[start_pos: start_pos + seq_len]\n",
    "\n",
    "        use_cache = kv_caches is not None\n",
    "        new_kv_caches = [] if use_cache else None\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer_kv_cache = kv_caches[i] if use_cache else None\n",
    "            if self.config.use_gradient_checkpointing and self.training:\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(h, freqs_cos, freqs_sin):\n",
    "                        return module(h, freqs_cos, freqs_sin, None, None, 0)\n",
    "                    return custom_forward\n",
    "                h_out, _ = gradient_checkpoint(\n",
    "                    create_custom_forward(layer), h, freqs_cos, freqs_sin,\n",
    "                    use_reentrant=False,\n",
    "                )\n",
    "                h = h_out\n",
    "                if use_cache:\n",
    "                    new_kv_caches.append(None)\n",
    "            else:\n",
    "                h, new_kv = layer(h, freqs_cos, freqs_sin, None, layer_kv_cache, start_pos)\n",
    "                if use_cache:\n",
    "                    new_kv_caches.append(new_kv)\n",
    "\n",
    "        h = self.norm(h)\n",
    "        logits = self.output(h)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1,\n",
    "            )\n",
    "\n",
    "        if use_cache:\n",
    "            return logits, loss, new_kv_caches\n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, learning_rate, weight_decay, betas, device):\n",
    "        decay_params = []\n",
    "        no_decay_params = []\n",
    "        for name, param in self.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "            if param.dim() >= 2:\n",
    "                decay_params.append(param)\n",
    "            else:\n",
    "                no_decay_params.append(param)\n",
    "\n",
    "        param_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "            {\"params\": no_decay_params, \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        n_decay = sum(p.numel() for p in decay_params)\n",
    "        n_no_decay = sum(p.numel() for p in no_decay_params)\n",
    "        print(f\"Optimizer: {n_decay:,} decay params, {n_no_decay:,} no-decay params\")\n",
    "\n",
    "        use_fused = device.type == \"cuda\"\n",
    "        return torch.optim.AdamW(param_groups, lr=learning_rate, betas=betas, fused=use_fused)\n",
    "\n",
    "\n",
    "print(\"Model architecture defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Tokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "def train_tokenizer(\n",
    "    input_file: str, model_prefix: str, vocab_size: int = 4096,\n",
    ") -> str:\n",
    "    \"\"\"Train a SentencePiece BPE tokenizer.\"\"\"\n",
    "    os.makedirs(os.path.dirname(model_prefix) or \".\", exist_ok=True)\n",
    "    print(f\"Training tokenizer (vocab_size={vocab_size})...\")\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        input=input_file, model_prefix=model_prefix, model_type=\"bpe\",\n",
    "        vocab_size=vocab_size, character_coverage=1.0, byte_fallback=True,\n",
    "        num_threads=4, max_sentence_length=4192, split_digits=True,\n",
    "        allow_whitespace_only_pieces=True, normalization_rule_name=\"identity\",\n",
    "        remove_extra_whitespaces=False, unk_id=0, bos_id=1, eos_id=2, pad_id=-1,\n",
    "    )\n",
    "    model_path = f\"{model_prefix}.model\"\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(model_path)\n",
    "    print(f\"Tokenizer trained: {model_path} (vocab={sp.GetPieceSize()})\")\n",
    "    # Verify roundtrip\n",
    "    test = \"Once upon a time, there was a little cat.\"\n",
    "    assert sp.Decode(sp.Encode(test)) == test, \"Roundtrip failed!\"\n",
    "    print(\"Roundtrip test: PASSED\")\n",
    "    return model_path\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Wrapper around a trained SentencePiece model.\"\"\"\n",
    "    def __init__(self, model_path: str):\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"Tokenizer model not found: {model_path}\")\n",
    "        self._sp = spm.SentencePieceProcessor()\n",
    "        self._sp.Load(model_path)\n",
    "\n",
    "    def encode(self, text: str, bos: bool = True, eos: bool = True) -> list:\n",
    "        tokens = self._sp.Encode(text)\n",
    "        if bos:\n",
    "            tokens = [self.bos_id] + tokens\n",
    "        if eos:\n",
    "            tokens = tokens + [self.eos_id]\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens: list) -> str:\n",
    "        return self._sp.Decode(tokens)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return self._sp.GetPieceSize()\n",
    "\n",
    "    @property\n",
    "    def bos_id(self) -> int:\n",
    "        return self._sp.bos_id()\n",
    "\n",
    "    @property\n",
    "    def eos_id(self) -> int:\n",
    "        return self._sp.eos_id()\n",
    "\n",
    "    @property\n",
    "    def unk_id(self) -> int:\n",
    "        return self._sp.unk_id()\n",
    "\n",
    "    @property\n",
    "    def pad_id(self) -> int:\n",
    "        pad = self._sp.pad_id()\n",
    "        return pad if pad >= 0 else -1\n",
    "\n",
    "    def id_to_piece(self, token_id: int) -> str:\n",
    "        return self._sp.IdToPiece(token_id)\n",
    "\n",
    "    def piece_to_id(self, piece: str) -> int:\n",
    "        return self._sp.PieceToId(piece)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.vocab_size\n",
    "\n",
    "\n",
    "print(\"Tokenizer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Data Pipeline\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def download_tinystories(data_dir: str) -> str:\n",
    "    \"\"\"Download TinyStories train split and export to text file.\"\"\"\n",
    "    from datasets import load_dataset\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    text_file = os.path.join(data_dir, \"tinystories_train.txt\")\n",
    "    if os.path.exists(text_file):\n",
    "        print(f\"Training text already exists: {text_file}\")\n",
    "        return text_file\n",
    "    print(\"Downloading TinyStories dataset...\")\n",
    "    dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "    print(f\"Downloaded {len(dataset):,} stories\")\n",
    "    with open(text_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for example in tqdm(dataset, desc=\"Exporting\"):\n",
    "            text = example[\"text\"].strip()\n",
    "            if text:\n",
    "                f.write(text + \"\\n\")\n",
    "    print(f\"Saved: {text_file} ({os.path.getsize(text_file) / 1024**2:.1f} MB)\")\n",
    "    return text_file\n",
    "\n",
    "\n",
    "def download_tinystories_val(data_dir: str) -> str:\n",
    "    \"\"\"Download TinyStories validation split.\"\"\"\n",
    "    from datasets import load_dataset\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    text_file = os.path.join(data_dir, \"tinystories_val.txt\")\n",
    "    if os.path.exists(text_file):\n",
    "        print(f\"Validation text already exists: {text_file}\")\n",
    "        return text_file\n",
    "    print(\"Downloading TinyStories validation split...\")\n",
    "    dataset = load_dataset(\"roneneldan/TinyStories\", split=\"validation\")\n",
    "    print(f\"Downloaded {len(dataset):,} validation stories\")\n",
    "    with open(text_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for example in tqdm(dataset, desc=\"Exporting val\"):\n",
    "            text = example[\"text\"].strip()\n",
    "            if text:\n",
    "                f.write(text + \"\\n\")\n",
    "    return text_file\n",
    "\n",
    "\n",
    "def tokenize_and_save(\n",
    "    text_file: str, output_bin: str, tokenizer: Tokenizer, chunk_size: int = 10000,\n",
    ") -> int:\n",
    "    \"\"\"Tokenize a text file and save as memory-mapped binary (uint16).\"\"\"\n",
    "    if os.path.exists(output_bin):\n",
    "        data = np.memmap(output_bin, dtype=np.uint16, mode=\"r\")\n",
    "        print(f\"Tokenized data already exists: {output_bin} ({len(data):,} tokens)\")\n",
    "        return len(data)\n",
    "    print(f\"Tokenizing {text_file} -> {output_bin}\")\n",
    "    with open(text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        n_lines = sum(1 for _ in f)\n",
    "    all_tokens = []\n",
    "    with open(text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, total=n_lines, desc=\"Tokenizing\"):\n",
    "            text = line.strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            tokens = tokenizer.encode(text, bos=False, eos=True)\n",
    "            all_tokens.extend(tokens)\n",
    "    token_array = np.array(all_tokens, dtype=np.uint16)\n",
    "    os.makedirs(os.path.dirname(output_bin) or \".\", exist_ok=True)\n",
    "    token_array.tofile(output_bin)\n",
    "    print(f\"Saved: {output_bin} ({len(token_array):,} tokens, {os.path.getsize(output_bin) / 1024**2:.1f} MB)\")\n",
    "    return len(token_array)\n",
    "\n",
    "\n",
    "def prepare_data(data_dir: str, tokenizer: Tokenizer) -> tuple:\n",
    "    \"\"\"Download and tokenize train/val splits. Returns (train_bin, val_bin) paths.\"\"\"\n",
    "    train_bin = os.path.join(data_dir, \"train.bin\")\n",
    "    val_bin = os.path.join(data_dir, \"val.bin\")\n",
    "    if not os.path.exists(train_bin):\n",
    "        train_text = download_tinystories(data_dir)\n",
    "        tokenize_and_save(train_text, train_bin, tokenizer)\n",
    "    else:\n",
    "        n = os.path.getsize(train_bin) // 2\n",
    "        print(f\"Train data ready: {train_bin} ({n:,} tokens)\")\n",
    "    if not os.path.exists(val_bin):\n",
    "        val_text = download_tinystories_val(data_dir)\n",
    "        tokenize_and_save(val_text, val_bin, tokenizer)\n",
    "    else:\n",
    "        n = os.path.getsize(val_bin) // 2\n",
    "        print(f\"Val data ready: {val_bin} ({n:,} tokens)\")\n",
    "    return train_bin, val_bin\n",
    "\n",
    "\n",
    "class TokenDataset(Dataset):\n",
    "    \"\"\"Dataset that reads from a pre-tokenized binary file.\"\"\"\n",
    "    def __init__(self, data_path: str, seq_len: int):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.data = np.memmap(data_path, dtype=np.uint16, mode=\"r\")\n",
    "        self.n_tokens = len(self.data)\n",
    "        print(f\"TokenDataset: {data_path} ({self.n_tokens:,} tokens, seq_len={seq_len})\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.n_tokens - self.seq_len - 1\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        chunk = self.data[idx: idx + self.seq_len + 1].astype(np.int64)\n",
    "        x = torch.from_numpy(chunk[:-1])\n",
    "        y = torch.from_numpy(chunk[1:])\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def create_dataloader(\n",
    "    data_path: str, seq_len: int, batch_size: int,\n",
    "    shuffle: bool = True, num_workers: int = 2, pin_memory: bool = True,\n",
    ") -> DataLoader:\n",
    "    \"\"\"Create a DataLoader for training or evaluation.\"\"\"\n",
    "    dataset = TokenDataset(data_path, seq_len)\n",
    "    return DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle,\n",
    "        num_workers=num_workers, pin_memory=pin_memory, drop_last=True,\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Data pipeline defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Utilities: Device, Checkpoints, Generation, Training Helpers\n",
    "\n",
    "# ── Device Utilities ─────────────────────────────────────────────────────\n",
    "\n",
    "def get_dtype(requested: str, device: torch.device) -> torch.dtype:\n",
    "    \"\"\"Resolve dtype string to optimal torch.dtype for the device.\"\"\"\n",
    "    if requested == \"auto\":\n",
    "        if device.type == \"cuda\":\n",
    "            return torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "        else:\n",
    "            return torch.float32\n",
    "    return {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[requested]\n",
    "\n",
    "\n",
    "def get_autocast_context(device: torch.device, dtype: torch.dtype):\n",
    "    \"\"\"Return the appropriate AMP autocast context manager.\"\"\"\n",
    "    if device.type == \"cuda\" and dtype in (torch.float16, torch.bfloat16):\n",
    "        return torch.amp.autocast(device_type=\"cuda\", dtype=dtype)\n",
    "    elif device.type == \"mps\":\n",
    "        return torch.amp.autocast(device_type=\"mps\", dtype=torch.float16)\n",
    "    return nullcontext()\n",
    "\n",
    "\n",
    "def get_grad_scaler(device: torch.device, dtype: torch.dtype):\n",
    "    \"\"\"Create GradScaler if needed (fp16 on CUDA).\"\"\"\n",
    "    if dtype == torch.float16 and device.type == \"cuda\":\n",
    "        return torch.amp.GradScaler(device=\"cuda\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_memory_usage(device: torch.device) -> dict:\n",
    "    if device.type == \"cuda\":\n",
    "        return {\n",
    "            \"allocated_mb\": torch.cuda.memory_allocated(device) / 1024**2,\n",
    "            \"reserved_mb\": torch.cuda.memory_reserved(device) / 1024**2,\n",
    "        }\n",
    "    return {\"allocated_mb\": 0.0, \"reserved_mb\": 0.0}\n",
    "\n",
    "\n",
    "# ── Reproducibility ──────────────────────────────────────────────────────\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# ── Model Diagnostics ────────────────────────────────────────────────────\n",
    "\n",
    "def count_parameters(model: nn.Module, trainable_only: bool = True) -> int:\n",
    "    if trainable_only:\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "def print_model_summary(model: nn.Module) -> str:\n",
    "    lines = [\"=\" * 65, \"Model Parameter Summary\", \"=\" * 65]\n",
    "    lines.append(f\"{'Name':<40} {'Params':>12} {'%':>7}\")\n",
    "    lines.append(\"-\" * 65)\n",
    "    param_list = [(name, p) for name, p in model.named_parameters()]\n",
    "    grand_total = sum(p.numel() for _, p in param_list)\n",
    "    trainable = 0\n",
    "    for name, param in param_list:\n",
    "        n = param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable += n\n",
    "        pct = 100.0 * n / grand_total if grand_total > 0 else 0\n",
    "        lines.append(f\"  {name:<38} {n:>12,d} ({pct:>5.1f}%)\")\n",
    "    lines.append(\"-\" * 65)\n",
    "    lines.append(f\"  {'TOTAL (trainable)':<38} {trainable:>12,d}\")\n",
    "    lines.append(f\"  {'Memory (fp32)':<38} {grand_total * 4 / 1024**2:>10.1f} MB\")\n",
    "    lines.append(f\"  {'Memory (fp16/bf16)':<38} {grand_total * 2 / 1024**2:>10.1f} MB\")\n",
    "    lines.append(\"=\" * 65)\n",
    "    summary = \"\\n\".join(lines)\n",
    "    print(summary)\n",
    "    return summary\n",
    "\n",
    "\n",
    "# ── Checkpoints ──────────────────────────────────────────────────────────\n",
    "\n",
    "def save_checkpoint(model, optimizer, step, val_loss, model_config, train_config, path):\n",
    "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"step\": step, \"val_loss\": val_loss,\n",
    "        \"model_config\": model_config, \"train_config\": train_config,\n",
    "        \"rng_state_python\": random.getstate(),\n",
    "        \"rng_state_numpy\": np.random.get_state(),\n",
    "        \"rng_state_torch\": torch.random.get_rng_state(),\n",
    "    }\n",
    "    if torch.cuda.is_available():\n",
    "        checkpoint[\"rng_state_cuda\"] = torch.cuda.get_rng_state_all()\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint saved: {path} (step {step}, val_loss {val_loss:.4f})\")\n",
    "\n",
    "\n",
    "def load_checkpoint(path, model, optimizer=None, device=None):\n",
    "    map_location = device if device else \"cpu\"\n",
    "    checkpoint = torch.load(path, map_location=map_location, weights_only=False)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    if optimizer is not None and \"optimizer_state_dict\" in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    if \"rng_state_python\" in checkpoint:\n",
    "        random.setstate(checkpoint[\"rng_state_python\"])\n",
    "    if \"rng_state_numpy\" in checkpoint:\n",
    "        np.random.set_state(checkpoint[\"rng_state_numpy\"])\n",
    "    if \"rng_state_torch\" in checkpoint:\n",
    "        torch.random.set_rng_state(checkpoint[\"rng_state_torch\"])\n",
    "    if \"rng_state_cuda\" in checkpoint and torch.cuda.is_available():\n",
    "        torch.cuda.set_rng_state_all(checkpoint[\"rng_state_cuda\"])\n",
    "    info = {\n",
    "        \"step\": checkpoint.get(\"step\", 0),\n",
    "        \"val_loss\": checkpoint.get(\"val_loss\", float(\"inf\")),\n",
    "        \"model_config\": checkpoint.get(\"model_config\", {}),\n",
    "    }\n",
    "    print(f\"Checkpoint loaded: {path} (step {info['step']})\")\n",
    "    return info\n",
    "\n",
    "\n",
    "# ── Generation ────────────────────────────────────────────────────────────\n",
    "\n",
    "def sample_top_p(probs: torch.Tensor, p: float) -> torch.Tensor:\n",
    "    probs_sorted, sorted_indices = torch.sort(probs, descending=True)\n",
    "    cumsum = torch.cumsum(probs_sorted, dim=-1)\n",
    "    mask = cumsum - probs_sorted > p\n",
    "    probs_sorted[mask] = 0.0\n",
    "    probs_sorted /= probs_sorted.sum()\n",
    "    sampled_idx = torch.multinomial(probs_sorted, num_samples=1)\n",
    "    return sorted_indices[sampled_idx]\n",
    "\n",
    "\n",
    "def _sample_token(logits, temperature, top_k, top_p):\n",
    "    logits = logits.squeeze(0)\n",
    "    if temperature == 0.0:\n",
    "        return logits.argmax()\n",
    "    logits = logits / temperature\n",
    "    if top_k > 0:\n",
    "        top_k = min(top_k, logits.size(-1))\n",
    "        kth_value = torch.topk(logits, top_k).values[-1]\n",
    "        logits[logits < kth_value] = float(\"-inf\")\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    if top_p < 1.0:\n",
    "        return sample_top_p(probs, top_p)\n",
    "    return torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate(\n",
    "    model, tokenizer, prompt, max_new_tokens=200,\n",
    "    temperature=0.8, top_k=40, top_p=0.9, device=None,\n",
    ") -> str:\n",
    "    \"\"\"Generate text from a prompt using KV-cached autoregressive decoding.\"\"\"\n",
    "    model.eval()\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    prompt_tokens = tokenizer.encode(prompt, bos=True, eos=False)\n",
    "    tokens = torch.tensor([prompt_tokens], dtype=torch.long, device=device)\n",
    "    prompt_len = tokens.shape[1]\n",
    "\n",
    "    # Prefill: process entire prompt\n",
    "    n_layers = model.config.n_layers\n",
    "    logits, _, kv_caches = model(tokens, kv_caches=[None] * n_layers, start_pos=0)\n",
    "    next_logits = logits[:, -1, :]\n",
    "    cur_pos = prompt_len\n",
    "\n",
    "    # Decode: generate one token at a time\n",
    "    generated_tokens = []\n",
    "    for _ in range(max_new_tokens):\n",
    "        next_token = _sample_token(next_logits, temperature, top_k, top_p)\n",
    "        generated_tokens.append(next_token.item())\n",
    "        if next_token.item() == tokenizer.eos_id:\n",
    "            break\n",
    "        new_token_tensor = next_token.unsqueeze(0).unsqueeze(0)\n",
    "        logits, _, kv_caches = model(\n",
    "            new_token_tensor, kv_caches=kv_caches, start_pos=cur_pos,\n",
    "        )\n",
    "        next_logits = logits[:, -1, :]\n",
    "        cur_pos += 1\n",
    "\n",
    "    all_tokens = prompt_tokens[1:] + generated_tokens  # Skip BOS\n",
    "    return tokenizer.decode(all_tokens)\n",
    "\n",
    "\n",
    "# ── Training Helpers ──────────────────────────────────────────────────────\n",
    "\n",
    "def get_lr(step, warmup_steps, max_steps, learning_rate, min_learning_rate):\n",
    "    \"\"\"Cosine annealing with linear warmup.\"\"\"\n",
    "    if step < warmup_steps:\n",
    "        return learning_rate * (step / warmup_steps)\n",
    "    if step >= max_steps:\n",
    "        return min_learning_rate\n",
    "    progress = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "    return min_learning_rate + (learning_rate - min_learning_rate) * 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_dataloader, device, autocast_ctx, max_steps=20):\n",
    "    \"\"\"Compute average validation loss.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_steps = 0\n",
    "    for x, y in val_dataloader:\n",
    "        if n_steps >= max_steps:\n",
    "            break\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with autocast_ctx:\n",
    "            _, loss = model(x, targets=y)\n",
    "        total_loss += loss.item()\n",
    "        n_steps += 1\n",
    "    model.train()\n",
    "    return total_loss / max(n_steps, 1)\n",
    "\n",
    "\n",
    "print(\"All utilities defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Train Tokenizer\n",
    "\n",
    "We train a **BPE tokenizer** (Byte-Pair Encoding) with SentencePiece on the TinyStories training data.\n",
    "\n",
    "- **Vocab size:** 4096 tokens (small, matching our tiny model)\n",
    "- **Byte fallback:** Unknown characters are encoded as UTF-8 bytes (no `<unk>` tokens)\n",
    "- **Special tokens:** `<s>` (BOS, id=1), `</s>` (EOS, id=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 1: Train Tokenizer\n",
    "\n",
    "# Download TinyStories training data (needed for tokenizer training)\n",
    "train_text_path = download_tinystories(DATA_DIR)\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer_model_path = TOKENIZER_PREFIX + \".model\"\n",
    "if os.path.exists(tokenizer_model_path):\n",
    "    print(f\"Tokenizer already exists: {tokenizer_model_path}\")\n",
    "else:\n",
    "    train_tokenizer(\n",
    "        input_file=train_text_path,\n",
    "        model_prefix=TOKENIZER_PREFIX,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "    )\n",
    "\n",
    "# Load and verify\n",
    "tokenizer = Tokenizer(tokenizer_model_path)\n",
    "print(f\"\\nTokenizer loaded: vocab_size={tokenizer.vocab_size}\")\n",
    "\n",
    "# Test roundtrip\n",
    "test_text = \"Once upon a time, there was a little cat.\"\n",
    "tokens = tokenizer.encode(test_text, bos=True, eos=True)\n",
    "decoded = tokenizer.decode(tokens)\n",
    "print(f\"Encode: '{test_text}'\")\n",
    "print(f\"  -> {tokens[:15]}... ({len(tokens)} tokens)\")\n",
    "print(f\"Decode: '{decoded}'\")\n",
    "assert decoded.strip() == test_text, \"Roundtrip failed!\"\n",
    "print(\"Roundtrip: PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 2: Prepare Data (Tokenize to Binary)\n",
    "\n",
    "# Download val split and tokenize both splits to .bin files\n",
    "train_bin, val_bin = prepare_data(DATA_DIR, tokenizer)\n",
    "\n",
    "# Print dataset stats\n",
    "train_tokens = os.path.getsize(train_bin) // 2  # uint16 = 2 bytes\n",
    "val_tokens = os.path.getsize(val_bin) // 2\n",
    "print(f\"\\nTrain tokens: {train_tokens:,}\")\n",
    "print(f\"Val tokens:   {val_tokens:,}\")\n",
    "print(f\"Train file:   {os.path.getsize(train_bin) / 1024**2:.1f} MB\")\n",
    "print(f\"Val file:     {os.path.getsize(val_bin) / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 3: Create Model\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# Build model config (update vocab_size from tokenizer)\n",
    "model_config = ModelConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    dim=DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    n_kv_heads=N_KV_HEADS,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    ")\n",
    "model_config.validate()\n",
    "\n",
    "# Create model\n",
    "model = LLaMA(model_config).to(device)\n",
    "n_params = count_parameters(model)\n",
    "print(f\"\\nModel parameters: {n_params:,}\")\n",
    "print_model_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train\n",
    "\n",
    "The training loop below runs for `TRAINING_STEPS` optimizer steps with:\n",
    "- **Mixed precision** (bf16 on Ampere+, fp16 on T4, fp32 on CPU)\n",
    "- **Gradient accumulation** (effective batch = `BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS`)\n",
    "- **Cosine LR schedule** with linear warmup\n",
    "- **Gradient clipping** to prevent explosions\n",
    "- **Periodic evaluation** on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 4: Train the Model\n",
    "\n",
    "# ── Setup ────────────────────────────────────────────────────────────────\n",
    "dtype = get_dtype(\"auto\", device)\n",
    "autocast_ctx = get_autocast_context(device, dtype)\n",
    "scaler = get_grad_scaler(device, dtype)\n",
    "print(f\"Training dtype: {dtype}\")\n",
    "print(f\"GradScaler: {'enabled' if scaler else 'disabled'}\")\n",
    "\n",
    "# ── DataLoaders ──────────────────────────────────────────────────────────\n",
    "train_loader = create_dataloader(\n",
    "    train_bin, seq_len=MAX_SEQ_LEN, batch_size=BATCH_SIZE,\n",
    "    shuffle=True, pin_memory=(device.type != \"cpu\"),\n",
    ")\n",
    "val_loader = create_dataloader(\n",
    "    val_bin, seq_len=MAX_SEQ_LEN, batch_size=BATCH_SIZE,\n",
    "    shuffle=False, pin_memory=(device.type != \"cpu\"),\n",
    ")\n",
    "\n",
    "# ── Optimizer ────────────────────────────────────────────────────────────\n",
    "optimizer = model.configure_optimizers(\n",
    "    learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY,\n",
    "    betas=(0.9, 0.95), device=device,\n",
    ")\n",
    "\n",
    "# ── Training Loop ────────────────────────────────────────────────────────\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "model.train()\n",
    "train_iter = iter(train_loader)\n",
    "tokens_per_step = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS * MAX_SEQ_LEN\n",
    "best_val_loss = float(\"inf\")\n",
    "train_losses = []\n",
    "\n",
    "print(f\"\\nStarting training: {TRAINING_STEPS} steps\")\n",
    "print(f\"Effective batch: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS} sequences = {tokens_per_step:,} tokens/step\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "pbar = tqdm(range(TRAINING_STEPS), desc=\"Training\", unit=\"step\")\n",
    "for step in pbar:\n",
    "    step_start = time.perf_counter()\n",
    "\n",
    "    # ── Learning Rate Schedule ───────────────────────────────────────────\n",
    "    lr = get_lr(step, WARMUP_STEPS, TRAINING_STEPS, LEARNING_RATE, MIN_LEARNING_RATE)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "    # ── Gradient Accumulation ────────────────────────────────────────────\n",
    "    accumulated_loss = 0.0\n",
    "    for micro_step in range(GRADIENT_ACCUMULATION_STEPS):\n",
    "        try:\n",
    "            x, y = next(train_iter)\n",
    "        except StopIteration:\n",
    "            train_iter = iter(train_loader)\n",
    "            x, y = next(train_iter)\n",
    "\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        with autocast_ctx:\n",
    "            _, loss = model(x, targets=y)\n",
    "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        accumulated_loss += loss.item()\n",
    "\n",
    "    # ── Gradient Clipping + Optimizer Step ───────────────────────────────\n",
    "    if scaler is not None:\n",
    "        scaler.unscale_(optimizer)\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "\n",
    "    if scaler is not None:\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    else:\n",
    "        optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # ── Timing ───────────────────────────────────────────────────────────\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    step_time = time.perf_counter() - step_start\n",
    "    tok_per_sec = tokens_per_step / step_time\n",
    "\n",
    "    # ── Progress Bar ─────────────────────────────────────────────────────\n",
    "    train_losses.append(accumulated_loss)\n",
    "    pbar.set_postfix(loss=f\"{accumulated_loss:.4f}\", lr=f\"{lr:.2e}\", tps=f\"{tok_per_sec:,.0f}\")\n",
    "\n",
    "    # ── Log ──────────────────────────────────────────────────────────────\n",
    "    if step % LOG_INTERVAL == 0:\n",
    "        mem = get_memory_usage(device)\n",
    "        print(\n",
    "            f\"step {step:>5d}/{TRAINING_STEPS} | \"\n",
    "            f\"loss {accumulated_loss:.4f} | \"\n",
    "            f\"lr {lr:.2e} | \"\n",
    "            f\"{tok_per_sec:>8,.0f} tok/s | \"\n",
    "            f\"mem {mem['allocated_mb']:>6.0f} MB\"\n",
    "        )\n",
    "\n",
    "    # ── Evaluation ───────────────────────────────────────────────────────\n",
    "    if step > 0 and step % EVAL_INTERVAL == 0:\n",
    "        val_loss = evaluate(model, val_loader, device, autocast_ctx, max_steps=EVAL_STEPS)\n",
    "        perplexity = math.exp(val_loss)\n",
    "        print(f\"{'─' * 60}\")\n",
    "        print(f\"EVAL step {step} | val_loss {val_loss:.4f} | perplexity {perplexity:.2f}\")\n",
    "        print(f\"{'─' * 60}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_checkpoint(\n",
    "                model, optimizer, step, val_loss,\n",
    "                model_config.to_dict(), {},\n",
    "                os.path.join(CHECKPOINT_DIR, \"best.pt\"),\n",
    "            )\n",
    "        model.train()\n",
    "\n",
    "    # ── Periodic Checkpoint ──────────────────────────────────────────────\n",
    "    if step > 0 and step % SAVE_INTERVAL == 0:\n",
    "        save_checkpoint(\n",
    "            model, optimizer, step, best_val_loss,\n",
    "            model_config.to_dict(), {},\n",
    "            os.path.join(CHECKPOINT_DIR, f\"step_{step:06d}.pt\"),\n",
    "        )\n",
    "\n",
    "# ── Final ────────────────────────────────────────────────────────────────\n",
    "print(\"\\nTraining complete!\")\n",
    "final_path = os.path.join(CHECKPOINT_DIR, \"final.pt\")\n",
    "save_checkpoint(\n",
    "    model, optimizer, TRAINING_STEPS, best_val_loss,\n",
    "    model_config.to_dict(), {}, final_path,\n",
    ")\n",
    "print(f\"Final checkpoint saved: {final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 5: Evaluate\n",
    "\n",
    "val_loss = evaluate(model, val_loader, device, autocast_ctx, max_steps=EVAL_STEPS)\n",
    "perplexity = math.exp(val_loss)\n",
    "\n",
    "print(f\"Final Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Final Perplexity:      {perplexity:.2f}\")\n",
    "print(f\"Best Validation Loss:  {best_val_loss:.4f}\")\n",
    "print(f\"Best Perplexity:       {math.exp(best_val_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 6: Generate Text\n",
    "\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The little dog\",\n",
    "    \"She looked at the sky and\",\n",
    "    \"One day, a boy named Tom\",\n",
    "]\n",
    "\n",
    "for temp in [0.7, 1.0]:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Temperature = {temp}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    for prompt in prompts:\n",
    "        text = generate(\n",
    "            model, tokenizer, prompt,\n",
    "            max_new_tokens=150, temperature=temp,\n",
    "            top_k=40, top_p=0.9, device=device,\n",
    "        )\n",
    "        print(f\"\\n--- Prompt: \\\"{prompt}\\\" ---\")\n",
    "        print(text)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 7: Save & Download Model\n",
    "\n",
    "# Save model config alongside checkpoint\n",
    "model_config.save(os.path.join(CHECKPOINT_DIR, \"model_config.json\"))\n",
    "print(f\"Model config saved to {CHECKPOINT_DIR}model_config.json\")\n",
    "\n",
    "# List all checkpoints\n",
    "print(\"\\nCheckpoints:\")\n",
    "for f in sorted(os.listdir(CHECKPOINT_DIR)):\n",
    "    path = os.path.join(CHECKPOINT_DIR, f)\n",
    "    size_mb = os.path.getsize(path) / 1024**2\n",
    "    print(f\"  {f}: {size_mb:.1f} MB\")\n",
    "\n",
    "# Download the best checkpoint (Colab only)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    best_path = os.path.join(CHECKPOINT_DIR, \"best.pt\")\n",
    "    if os.path.exists(best_path):\n",
    "        print(f\"\\nDownloading {best_path}...\")\n",
    "        files.download(best_path)\n",
    "    else:\n",
    "        print(f\"\\nDownloading final checkpoint...\")\n",
    "        files.download(os.path.join(CHECKPOINT_DIR, \"final.pt\"))\n",
    "except ImportError:\n",
    "    print(\"\\nNot running on Colab — skipping download.\")\n",
    "    print(f\"Checkpoints are at: {os.path.abspath(CHECKPOINT_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Bonus: Load Checkpoint & Generate (proves the save works)\n",
    "\n",
    "# Create a fresh model from config\n",
    "loaded_config = ModelConfig.load(os.path.join(CHECKPOINT_DIR, \"model_config.json\"))\n",
    "loaded_model = LLaMA(loaded_config).to(device)\n",
    "\n",
    "# Load the best checkpoint\n",
    "ckpt_path = os.path.join(CHECKPOINT_DIR, \"best.pt\")\n",
    "if not os.path.exists(ckpt_path):\n",
    "    ckpt_path = os.path.join(CHECKPOINT_DIR, \"final.pt\")\n",
    "\n",
    "info = load_checkpoint(ckpt_path, loaded_model, device=device)\n",
    "print(f\"Loaded checkpoint from step {info['step']}, val_loss {info['val_loss']:.4f}\")\n",
    "\n",
    "# Generate with loaded model\n",
    "print(\"\\n--- Generation from loaded checkpoint ---\")\n",
    "for prompt in [\"Once upon a time\", \"The little cat was\"]:\n",
    "    text = generate(\n",
    "        loaded_model, tokenizer, prompt,\n",
    "        max_new_tokens=100, temperature=0.8, device=device,\n",
    "    )\n",
    "    print(f\"\\nPrompt: \\\"{prompt}\\\"\")\n",
    "    print(text)\n",
    "\n",
    "print(\"\\nCheckpoint load + generate: SUCCESS\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}